# RAG System with LangChain & Ollama

## Description
AI-powered question-answering system built with **RAG (Retrieval-Augmented Generation)** architecture. The system uses **LangChain** and **LangGraph** for agent orchestration, **Ollama** for local LLM inference, **FAISS** for vector storage, and **Sentence Transformers** for embeddings.

### Key Features
- ğŸ¤– **LangGraph-based Agent** - Modular RAG workflow with retrieve â†’ prompt â†’ generate pipeline
- ğŸ  **Local LLM** - Runs on Ollama (Qwen2.5:7b) - no API costs
- ğŸŒ **Web UI** - Interactive chat interface via LangServe Playground
- ğŸ“š **Document Processing** - Supports PDF and TXT files with advanced preprocessing
- ğŸ” **Semantic Search** - FAISS vector database with similarity search
- ğŸ’¬ **CLI Mode** - Interactive terminal interface for quick queries

## Prerequisites

1. **Install Ollama** and download the model:
   ```bash
   # Install Ollama from https://ollama.ai
   ollama pull qwen2.5:7b
   ```

2. **Python 3.8+** with virtual environment support

## Installation

1. Clone the repository:
   ```bash
   git clone <repo-url>
   cd RAG-system
   ```

2. Create and activate virtual environment:
   ```bash
   python -m venv .venv
   .venv\Scripts\activate      # Windows
   source .venv/bin/activate   # Linux/Mac
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Configure environment variables (optional):
   ```bash
   # Edit .env file if needed
   # Default values work out of the box
   ```

## Quick Start

### 1. Index Your Documents

Place PDF or TXT files in `data/raw/`, then run:

```bash
python main.py --mode index
```

This will:
- Process and clean documents
- Generate embeddings using Sentence Transformers
- Store vectors in FAISS index at `data/indexes/knowledge_base`

### 2. Start Ollama server

```bash
ollama serve
```

### 3. Start the Web Interface

```bash
python server.py
```

Open your browser at: **http://localhost:8000/rag/playground/**

You'll see an interactive chat interface where you can ask questions about your documents.

### 4. CLI Mode (Alternative)

For terminal-based interaction:

```bash
python main.py --mode interactive
```

Or single query mode:

```bash
python main.py --mode query --question "Ğ©Ğ¾ Ñ‚Ğ°ĞºĞµ Ğ¼Ñ–ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ñ–ÑĞ¸?"
```

## Project Structure

```
RAG-system/
â”‚
â”œâ”€â”€ server.py              # LangServe web server entry point
â”œâ”€â”€ main.py                # CLI entry point
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env                   # Environment configuration
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/             # LangGraph agent & LLM clients
â”‚   â”‚   â”œâ”€â”€ agent.py       # AIAgent with LangGraph workflow
â”‚   â”‚   â”œâ”€â”€ llm_client.py  # Ollama & Perplexity clients
â”‚   â”‚   â”œâ”€â”€ retriever.py   # Document retrieval logic
â”‚   â”‚   â””â”€â”€ prompt_builder.py
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/     # Document processing pipeline
â”‚   â”œâ”€â”€ embeddings/        # Sentence Transformers embedder
â”‚   â”œâ”€â”€ storage/           # FAISS vector storage
â”‚   â””â”€â”€ models.py          # Data models
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Place your documents here
â”‚   â””â”€â”€ indexes/           # Generated FAISS indexes
â”‚
â”œâ”€â”€ prompts/               # System prompts
â”œâ”€â”€ config/                # Logging configuration
â””â”€â”€ tests/                 # Unit tests
```

## Architecture

The system uses **LangGraph** to orchestrate the RAG workflow:

```
User Query â†’ Retrieve (FAISS) â†’ Build Prompt â†’ Generate (Ollama) â†’ Response
```

### Components

1. **Retriever** - Finds relevant document chunks using semantic search
2. **Prompt Builder** - Constructs context-aware prompts
3. **LLM Client** - Generates answers via Ollama (local) or Perplexity API
4. **Agent** - Orchestrates the workflow using LangGraph

## Configuration

Edit `.env` to customize:

```bash
# LLM Settings
LLM_PROVIDER=ollama
LLM_MODEL=qwen2.5:7b
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=800

# Embeddings
EMBEDDER_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Retrieval
TOP_K=5
MIN_SIMILARITY=0.3

# Chunking
CHUNK_SIZE=800
CHUNK_OVERLAP=150
```

## API Documentation

When `server.py` is running, visit:
- **Playground**: http://localhost:8000/rag/playground/
- **API Docs**: http://localhost:8000/docs

## Development

### Running Tests

```bash
pytest tests/ --cov=src
```

### Git Workflow

```bash
git checkout -b feature/your-feature
# Make changes
git commit -m "Description"
git push origin feature/your-feature
```

## Technologies

- **LangChain** & **LangGraph** - Agent framework
- **Ollama** - Local LLM runtime
- **FAISS** - Vector database
- **Sentence Transformers** - Embeddings
- **FastAPI** - Web server
- **LangServe** - LangChain deployment

## Troubleshooting

### "Index not found"
Run `python main.py --mode index` first to create the index.

### "Ollama connection error"
Make sure Ollama is running: `ollama serve`

### Empty responses in web UI
Check server logs for `DEBUG:` messages. Restart server after code changes.

## License

MIT
