import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.preprocessing.preprocessor import Preprocessor
from src.embeddings.embedder import EmbedderFactory
from src.storage.storage import FAISSStorage
from src.models import ProcessorResult, TextChunk


def test_full_pipeline():
    """–¢–µ—Å—Ç –ø–æ–≤–Ω–æ–≥–æ RAG pipeline"""
    print("=" * 70)
    print("–Ü–ù–¢–ï–ì–†–ê–¶–Ü–ô–ù–ò–ô –¢–ï–°–¢: Preprocessor -> Chunker -> Embedder -> Storage")
    print("=" * 70)

    # 1. –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
    preprocessor = Preprocessor()
    embedder = EmbedderFactory.create(method="sbert")
    storage = FAISSStorage(dimension=384)

    print("‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ —Å—Ç–≤–æ—Ä–µ–Ω–æ\n")

    file_name = "introduction_to_microservices_galkin_shkilniak"
    path = f"data\\raw\\{file_name}.pdf"
    result = preprocessor.process_document(path)

    # 3. –í–µ–∫—Ç–æ—Ä–∏–∑—É—î–º–æ
    embeddings = embedder.embed_batch(result.chunks)
    print(
        f"üî¢ –°—Ç–≤–æ—Ä–µ–Ω–æ {len(embeddings)} –≤–µ–∫—Ç–æ—Ä—ñ–≤ (dim={len(embeddings[0].vector)})\n"
    )

    # 4. –î–æ–¥–∞—î–º–æ –≤ storage
    storage.add(embeddings)
    stats = storage.get_stats()
    print(f"üíæ –Ü–Ω–¥–µ–∫—Å —Å—Ç–≤–æ—Ä–µ–Ω–æ:")
    print(f"   - –í–µ–∫—Ç–æ—Ä—ñ–≤: {stats['total_vectors']}")
    print(f"   - –î–æ–∫—É–º–µ–Ω—Ç—ñ–≤: {stats['unique_documents']}")
    print(f"   - –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {stats['dimension']}\n")

    # 5. –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞ –¥–∏—Å–∫
    storage.save("test_knowledge_base")
    print("üíæ –Ü–Ω–¥–µ–∫—Å –∑–±–µ—Ä–µ–∂–µ–Ω–æ –Ω–∞ –¥–∏—Å–∫\n")

    # 6. –í–∏–∫–æ–Ω—É—î–º–æ –ø–æ—à—É–∫
    query_text = "–ë—ñ–∑–Ω–µ—Å-–º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ?"
    print(f"üîç –ó–∞–ø–∏—Ç: '{query_text}'")

    # –í–µ–∫—Ç–æ—Ä–∏–∑—É—î–º–æ –∑–∞–ø–∏—Ç
    query_chunk = TextChunk(text=query_text,
                            chunk_id="query",
                            document_id="query")
    query_embedding = embedder.embed(query_chunk)

    # –®—É–∫–∞—î–º–æ
    results = storage.search(query_embedding.vector, top_k=3)

    print(f"\nüìä –ó–Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤:\n")
    for i, result in enumerate(results):
        print(f"{i+1}. Score: {result.score:.4f}")
        print(f"   Text: {result.chunk.text[:80]}...")
        print(f"   Chunk ID: {result.chunk_id}")
        print(f"   Doc: {result.document_id}\n")

    # 7. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —ñ–Ω–¥–µ–∫—Å –∑ –¥–∏—Å–∫—É
    storage2 = FAISSStorage()
    storage2.load("test_knowledge_base")
    results2 = storage2.search(query_embedding.vector, top_k=3)
    assert len(results2) == len(results)
    print("‚úÖ –Ü–Ω–¥–µ–∫—Å —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ –¥–∏—Å–∫—É\n")

    # –û—á–∏—â–∞—î–º–æ
    import os
    os.remove("test_knowledge_base.faiss")
    os.remove("test_knowledge_base.pkl")

    print("=" * 70)
    print("üéâ –Ü–ù–¢–ï–ì–†–ê–¶–Ü–ô–ù–ò–ô –¢–ï–°–¢ –ü–†–û–ô–î–ï–ù–û –£–°–ü–Ü–®–ù–û!")
    print("=" * 70)


if __name__ == "__main__":
    test_full_pipeline()
